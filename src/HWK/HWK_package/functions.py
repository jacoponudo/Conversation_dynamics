
# Jacopo's functions for fitting hawkes process

import numpy as np
from scipy.optimize import minimize
from tqdm import tqdm
from scipy.stats import chi2

#### Max Likelihood Estiamtion #### 
# The following are 3 functions to estimate, given the realizations of a process, the parameters:
# -individual_exp_log_likelihood: calculates the individual log likelihood givene an observation.
# -sum_individual_exp_log_likelihood: sum all the likelihood functions on different observations.
# -considering the previous function as the objective function minimize the loss, with respect to the combination of parameters. 
    

def individual_exp_log_likelihood(event_times, T, parameters):
    """
    Calculate the individual log-likelihood of exponential survival model for a single event sequence.

    Args:
        event_times (array-like): Time of events.
        T (float): Total observation time.
        parameters (tuple): Tuple of parameters (Î», Î±, Î²).
 
    Returns:
        float: Log-likelihood of the given event sequence.
    """
    Î», Î±, Î² = parameters
    times = event_times
    N_T = len(times)

    A = np.empty(N_T, dtype=np.float64)
    A[0] = 0
    for i in range(1, N_T):
        A[i] = np.exp(-Î² * (times[i] - times[i - 1])) * (1 + A[i - 1])

    likelihood = -Î» * T
    for i, t_i in enumerate(event_times):
        likelihood += np.log(Î» + (Î±) * A[i]) - ((Î±) / Î²) * (1 - np.exp(-Î² * (T - t_i)))
    return likelihood


def sum_individual_exp_log_likelihood(event_sequences, T, parameters):
    """
    Calculate the sum of individual log-likelihoods of exponential survival model for multiple event sequences.

    Args:
        event_sequences (list): List of event sequences.
        T (float): Total observation time.
        parameters (tuple): Tuple of parameters (Î», Î±, Î²).

    Returns:
        float: Sum of log-likelihoods of the given event sequences.
    """
    total_likelihood = 0
    num_sequences = len(event_sequences)
    for i in (range(num_sequences)):
        likelihood = individual_exp_log_likelihood(event_sequences[i], T[i], parameters)
        total_likelihood += likelihood
    return total_likelihood


def exponential_mle(event_times, T, initial_parameters=np.array([0.5, 0, .1])):
    """
    Estimate Maximum Likelihood Estimates (MLE) for parameters of the exponential survival model.

    Args:
        event_times (array-like): Time of events.
        T (float): Total observation time.
        initial_parameters (array-like, optional): Initial guess for parameters. Defaults to [1.0, 2.0, 3.0].

    Returns:
        array-like: MLE estimates for parameters (Î», Î±, Î²).
    """
    eps = 0.00001e-300
    parameter_bounds = ((eps, None), (eps, None), (eps, None))
    loss_function = lambda parameters: -sum_individual_exp_log_likelihood(event_times, T, parameters)
    mle_parameters = minimize(loss_function, initial_parameters, bounds=parameter_bounds).x
    return np.array(mle_parameters)

#### Max Likelihood Estiamtion + Toxicity parameter #### 
# The following are 3 functions to estimate, given the realizations of a process, the parameters:
# -individual_exp_log_likelihood: calculates the individual log likelihood givene an observation.
# -sum_individual_exp_log_likelihood: sum all the likelihood functions on different observations.
# -considering the previous function as the objective function minimize the loss, with respect to the combination of parameters (considering also the alpha parameter of excitement driven by toxicity). 

def individual_exp_log_likelihood_toxicity(event_times, magnitude, T, parameters):
    """
    Calculate the individual log-likelihood of exponential survival model considering toxicity for a single event sequence.

    Args:
        event_times (array-like): Time of events.
        magnitude (array-like): Magnitude of toxicity for each event.
        T (float): Total observation time.
        parameters (tuple): Tuple of parameters (Î», Î±, Î±_T, Î²).

    Returns:
        float: Log-likelihood of the given event sequence considering toxicity.
    """
    Î», Î±, Î±_T, Î² = parameters
    times = event_times
    N_T = len(times)

    A = np.empty(N_T, dtype=np.float64)
    A[0] = 0
    for i in range(1, N_T):
        A[i] = np.exp(-Î² * (times[i] - times[i - 1])) * (1 + A[i - 1])

    likelihood = -Î» * T
    for i, t_i in enumerate(event_times):
        mi = magnitude[i]
        likelihood += np.log(Î» + (Î± + Î±_T * mi) * A[i]) - ((Î± + Î±_T * mi) / Î²) * (1 - np.exp(-Î² * (T - t_i)))
    return likelihood


def sum_individual_exp_log_likelihood_toxicity(event_sequences, magnitude_sequences, T, parameters):
    """
    Calculate the sum of individual log-likelihoods of exponential survival model considering toxicity for multiple event sequences.

    Args:
        event_sequences (list): List of event sequences.
        magnitude_sequences (list): List of magnitude sequences corresponding to event sequences.
        T (float): Total observation time.
        parameters (tuple): Tuple of parameters (Î», Î±, Î±_T, Î²).

    Returns:
        float: Sum of log-likelihoods of the given event sequences considering toxicity.
    """
    total_likelihood = 0
    num_sequences = len(event_sequences)
    for i in (range(num_sequences)):
        likelihood = individual_exp_log_likelihood_toxicity(event_sequences[i], magnitude_sequences[i], T[i], parameters)
        total_likelihood += likelihood
    return total_likelihood


def exponential_mle_toxicity(event_times, magnitude, T, initial_parameters=np.array([0.5, 0, 0, 0.1])):
    """
    Estimate Maximum Likelihood Estimates (MLE) for parameters of the exponential survival model considering toxicity.

    Args:
        event_times (array-like): Time of events.
        magnitude (array-like): Magnitude of toxicity for each event.
        T (float): Total observation time.
        initial_parameters (array-like, optional): Initial guess for parameters. Defaults to [1.0, 2.0, 5.0, 3.0].

    Returns:
        array-like: MLE estimates for parameters (Î», Î±, Î±_T, Î²).
    """
    eps = 0.00001e-300
    parameter_bounds = ((eps, None), (eps, None), (eps, None), (eps, None))
    loss_function = lambda parameters: -sum_individual_exp_log_likelihood_toxicity(event_times, magnitude, T, parameters)
    mle_parameters = minimize(loss_function, initial_parameters, bounds=parameter_bounds).x
    return np.array(mle_parameters)


#### Data Preparation ####
# Function to prepare data, given: data from user, data from all users, and list of conversations to focus on.
# return â„‹_T_list (list of list with timestamp of each single comment in a process standarsized among 0 and 1), magnitude_list (list of list of manginute of toxicity for each comment.) 

def prepare_data(df, dataset):
    """
    Filter toxic comments based on selected conversations and prepare data.

    Args:
    - df (DataFrame): DataFrame containing the user data
    - dataset (DataFrame): Complete dataset containing all conversations

    Returns:
    - â„‹_T_list (list): List of standardized conversation timestamps
    - magnitude_list (list): List of toxicity scores
    """

    # Filter toxic comments based on selected conversations
    df.sort_values(by='created_at', inplace=True)
    
    â„‹_T_list = []
    magnitude_list = []
    time_list=[]
    for conversation in (df['root_submission'].unique()):
        data_user_root=df[df['root_submission']==conversation]
        
        observed_data = np.array([np.datetime64(x.replace(tzinfo=None)).astype(np.int64) for x in data_user_root['created_at']])
        mean_lag=np.mean(np.diff(observed_data))
        â„‹_t=observed_data-min(observed_data)+mean_lag
                         
        magnitude = np.array(data_user_root['toxicity_score'])
        
        â„‹_T_list.append(â„‹_t)
        magnitude_list.append(magnitude)
        time_list.append(max(â„‹_t)+mean_lag)

    return â„‹_T_list, magnitude_list,time_list

def filter_dataset(dataset, username, min_comments,sample):
    """
    Filter dataset for a specific user with a minimum number of comments.

    Args:
        dataset (DataFrame): Input dataset.
        username (str): Username to filter.
        min_comments (int): Minimum number of comments.

    Returns:
        DataFrame: Filtered dataset.
    """
    # Filter data for the specified user
    df = dataset[dataset['user'] == username]
    
    # Group conversations by root_submission and count the number of comments
    grouped_data = df.groupby('root_submission').size().reset_index(name='num_comments')
    
    # Filter conversations with more than min_comments
    if sample==False:
        conversations = grouped_data[grouped_data['num_comments'] > min_comments]
    else:
        conversations = grouped_data[grouped_data['num_comments'] > min_comments].sample(sample,replace=True)
    
    # Filter toxic comments based on selected conversations
    data = df[df['root_submission'].isin(conversations['root_submission'])]
    
    return data


#### Model fitting evalution ####
def calculate_aic(n, k, likelihood):
    """
    Calculate the Akaike Information Criterion (AIC) for a given likelihood and number of parameters.

    Args:
        n (int): Number of observations.
        k (int): Number of parameters.
        likelihood (float): Log-likelihood of the model.

    Returns:
        float: AIC value.
    """
    return 2 * k - 2 * likelihood

def calculate_bic(n, k, likelihood):
    """
    Calculate the Bayesian Information Criterion (BIC) for a given likelihood, number of parameters, and number of observations.

    Args:
        n (int): Number of observations.
        k (int): Number of parameters.
        likelihood (float): Log-likelihood of the model.

    Returns:
        float: BIC value.
    """
    return k * np.log(n) - 2 * likelihood



#### Select subsample of users ####

def select_users_with_multiple_comments(data, min_comments_per_post=3, min_post_count=3):
    """
    Selects users who have made more than 'min_comments_per_post' comments under the same post
    for at least 'min_post_count' times.

    Args:
    - data: DataFrame containing the data
    - min_comments_per_post: Minimum number of comments under the same post to be considered
    - min_post_count: Minimum number of posts that satisfy the above criteria

    Returns:
    - DataFrame containing the users who satisfy the criteria
    """
    # Calculate the count of comments for each user and post
    comment_count = data.groupby(['user', 'root_submission']).size().reset_index(name='comment_count')

    # Filter the results for users who have made more than 'min_comments_per_post' comments under the same post
    filtered_data = comment_count[comment_count['comment_count'] > min_comments_per_post]

    # Select only the users who have satisfied the criteria for at least 'min_post_count' times
    final_result = filtered_data.groupby('user').filter(lambda x: len(x) >= min_post_count)['user'].unique()


    return final_result

#### Perform analysis, fitting the model ####
# Given a sample of users, we fit the simple Hawkes process and the version with the alpha of toxicity. 

def analyze_users(dataset, sample_users, min_comments):
    """
    Analyzes users based on a given dataset, a sample of users, and a minimum number of comments.

    Parameters:
        dataset (DataFrame): The dataset containing user comments.
        sample_users (list): A list of users to analyze.
        min_comments (int): The minimum number of comments required for analysis.

    Returns:
        list: A list of dictionaries containing the analysis results for each user.
    """
    user_analysis = []
    
    for user in tqdm(sample_users):
        # Filter comments by the user with at least the minimum number of comments
        toxic = filter_dataset(dataset, user, min_comments=min_comments, sample=False)
        
        # Prepare data for each conversation including timestamp of events and magnitude for each comment
        â„‹_T_list, magnitude_list = prepare_data(toxic, dataset)
        
        # Estimate parameters for toxicity model
        Î¸_exp_mle_T = exponential_mle_toxicity(â„‹_T_list, magnitude_list, 1)
        
        # Estimate parameters for simple model
        Î¸_exp_mle = exponential_mle(â„‹_T_list, 1)
        
        # Compute likelihood values for both models
        Lh_simple_model = sum_individual_exp_log_likelihood(â„‹_T_list, 1, Î¸_exp_mle)
        Lh_toxicity_model = sum_individual_exp_log_likelihood_toxicity(â„‹_T_list, magnitude_list, 1, Î¸_exp_mle_T)
        
        # Calculate size parameter alpha_T0 for the toxicity model
        size_alpha_T0 = Î¸_exp_mle_T[2] - Î¸_exp_mle_T[1] / Î¸_exp_mle_T[1]
        
        # Calculate AIC values for both models
        n = len(â„‹_T_list)
        k_T = len(Î¸_exp_mle_T)
        k = len(Î¸_exp_mle)
        aic_T = calculate_aic(n, k, Lh_toxicity_model)
        aic = calculate_aic(n, k_T, Lh_simple_model)
        
        # Calculate likelihood ratio and p-value
        log_likelihood_full = -sum_individual_exp_log_likelihood_toxicity(â„‹_T_list, magnitude_list, 1, Î¸_exp_mle_T)
        log_likelihood_simple = -sum_individual_exp_log_likelihood(â„‹_T_list, 1, Î¸_exp_mle)
        LR = 2 * (log_likelihood_full - log_likelihood_simple)
        df = len(Î¸_exp_mle_T) - len(Î¸_exp_mle)
        p_value = 1 - chi2.cdf(LR, df)
        
        # Check significance of likelihood test
        if p_value < 0.05:
            significance = "The full model provides better data fit than the simpler model (p-value < 0.05)"
        else:
            significance = "The full model does not provide better data fit than the simpler model (p-value >= 0.05)"
        
        # Append user analysis data to list
        user_analysis.append({'User': user,
                              'Lambda':Î¸_exp_mle[0],
                              'Lambda_T':Î¸_exp_mle_T[0],
                              'Alpha_1':Î¸_exp_mle[1],
                              'Alpha_1_T':Î¸_exp_mle_T[1],
                              'Alpha_2_T':Î¸_exp_mle_T[2],
                              'Beta':Î¸_exp_mle[2],
                              'Beta_T':Î¸_exp_mle_T[3],
                              'AIC_Toxicity_Model': aic_T,
                              'AIC_Simple_Model': aic,
                              'Number_of_Comments': len(toxic),
                              'P_Value_Likelihood_Test': p_value,
                              'Significance': significance})
    
    return user_analysis


# from hawkes

# -*- coding: utf-8 -*-
import numpy as np
import numpy.random as rnd
from scipy.optimize import fsolve, minimize

from tqdm import tqdm
from numba import njit, prange


@njit()
def numba_seed(seed):
    rnd.seed(seed)


# Intensities and compensators


def hawkes_intensity(t, â„‹_t, ğ›‰):
    Î», Î¼, _ = ğ›‰
    Î»Ë£ = Î»
    for t_i in â„‹_t:
        Î»Ë£ += Î¼(t - t_i)
    return Î»Ë£


def hawkes_compensator(t, â„‹_t, ğ›‰):
    if t <= 0: return 0
    Î», _, M = ğ›‰

    Î› = Î» * t
    for t_i in â„‹_t:
        Î› += M(t - t_i)
    return Î›


def exp_hawkes_intensity(t, â„‹_t, ğ›‰):
    Î», Î±, Î² = ğ›‰
    Î»Ë£ = Î»
    for t_i in â„‹_t:
        Î»Ë£ += Î± * np.exp(-Î² * (t - t_i))
    return Î»Ë£


def exp_hawkes_compensator(t, â„‹_t, ğ›‰):
    if t <= 0: return 0
    Î», Î±, Î² = ğ›‰
    Î› = Î» * t
    for t_i in â„‹_t:
        Î› += (Î±/Î²) * (1 - np.exp(-Î²*(t - t_i)))
    return Î›


@njit(nogil=True)
def exp_hawkes_compensators(â„‹_t, ğ›‰):
    Î», Î±, Î² = ğ›‰

    Î› = 0
    Î»Ë£_prev = Î»
    t_prev = 0

    Î›s = np.empty(len(â„‹_t), dtype=np.float64)
    for i, t_i in enumerate(â„‹_t):
        Î› += Î» * (t_i - t_prev) + (
                (Î»Ë£_prev - Î»)/Î² *
                (1 - np.exp(-Î²*(t_i - t_prev))))
        Î›s[i] = Î›

        Î»Ë£_prev = Î» + (Î»Ë£_prev - Î») * (
                np.exp(-Î² * (t_i - t_prev))) + Î±
        t_prev = t_i
    return Î›s


@njit(nogil=True)
def power_hawkes_intensity(t, â„‹_t, ğ›‰):
    Î», k, c, p = ğ›‰
    Î»Ë£ = Î»
    for t_i in â„‹_t:
        Î»Ë£ += k / (c + (t-t_i))**p
    return Î»Ë£


@njit(nogil=True)
def power_hawkes_compensator(t, â„‹_t, ğ›‰):
    Î», k, c, p = ğ›‰
    Î› = Î» * t
    for t_i in â„‹_t:
        Î› += ((k * (c * (c + (t-t_i)))**-p *
              (-c**p * (c + (t-t_i)) + c * (c + (t-t_i))**p)) /
              (p - 1))
    return Î›


@njit(nogil=True, parallel=True)
def power_hawkes_compensators(â„‹_t, ğ›‰):
    Î›s = np.empty(len(â„‹_t), dtype=np.float64)
    for i in prange(len(â„‹_t)):
        t_i = â„‹_t[i]
        â„‹_i = â„‹_t[:i]
        Î›s[i] = power_hawkes_compensator(t_i, â„‹_i, ğ›‰)
    return Î›s


# Likelihood

def log_likelihood(â„‹_T, T, ğ›‰, Î»Ë£, Î›):
    â„“ = 0.0
    for i, t_i in enumerate(â„‹_T):
        â„‹_i = â„‹_T[:i]
        Î»Ë£_i = Î»Ë£(t_i, â„‹_i, ğ›‰)
        â„“ += np.log(Î»Ë£_i)
    â„“ -= Î›(T, â„‹_T, ğ›‰)
    return â„“


@njit(nogil=True, parallel=True)
def power_log_likelihood(â„‹_T, T, ğ›‰):
    â„“ = 0.0
    for i in prange(len(â„‹_T)):
        t_i = â„‹_T[i]
        â„‹_i = â„‹_T[:i]
        Î»Ë£_i = power_hawkes_intensity(t_i, â„‹_i, ğ›‰)
        â„“ += np.log(Î»Ë£_i)
    â„“ -= power_hawkes_compensator(T, â„‹_T, ğ›‰)
    return â„“


@njit()
def exp_log_likelihood(â„‹_T, T, ğ›‰):
    Î», Î±, Î² = ğ›‰
    ğ­ = â„‹_T
    N_T = len(ğ­)

    A = np.empty(N_T, dtype=np.float64)
    A[0] = 0
    for i in range(1, N_T):
        A[i] = np.exp(-Î²*(ğ­[i] - ğ­[i-1])) * (1 + A[i-1])

    â„“ = -Î»*T
    for i, t_i in enumerate(â„‹_T):
        â„“ += np.log(Î» + Î± * A[i]) - \
                (Î±/Î²) * (1 - np.exp(-Î²*(T-t_i)))
    return â„“


def exp_mle(ğ­, T, ğ›‰_start=np.array([0.0001, 0.00001,.00001])):
    eps = 0.1e-300
    ğ›‰_bounds = ((eps, None), (eps, None), (eps, None))
    loss = lambda ğ›‰: -exp_log_likelihood(ğ­, T, ğ›‰)
    ğ›‰_mle = minimize(loss, ğ›‰_start, bounds=ğ›‰_bounds).x
    return np.array(ğ›‰_mle)


def power_mle(ğ­, T, ğ›‰_start=np.array([1.0, 1.0, 2.0, 3.0])):
    eps = 1e-5
    ğ›‰_bounds = ((eps, None), (eps, None), (eps, None),
        (1+eps, 100))
    loss = lambda ğ›‰: -power_log_likelihood(ğ­, T, ğ›‰)
    ğ›‰_mle = minimize(loss, ğ›‰_start, bounds=ğ›‰_bounds).x
    return np.array(ğ›‰_mle)


# Simulation


def simulate_inverse_compensator(ğ›‰, Î›, N):
    â„‹ = np.empty(N, dtype=np.float64)

    tË£_1 = -np.log(rnd.rand())
    exp_1 = lambda t_1: Î›(t_1, â„‹[:0], ğ›‰) - tË£_1

    t_1_guess = 1.0
    t_1 = fsolve(exp_1, t_1_guess)[0]

    â„‹[0] = t_1
    t_prev = t_1
    for i in range(1, N):
        Î”tË£_i = -np.log(rnd.rand())

        Î›_i = Î›(t_prev, â„‹, ğ›‰)
        exp_i = lambda t_next: Î›(t_next, â„‹[:i], ğ›‰) - Î›_i - Î”tË£_i

        t_next_guess = t_prev + 1.0
        t_next = fsolve(exp_i, t_next_guess)[0]

        â„‹[i] = t_next
        t_prev = t_next
    return â„‹

@njit(nogil=True)
def exp_simulate_by_composition(ğ›‰, N):
    Î», Î±, Î² = ğ›‰
    Î»Ë£_k = Î»
    t_k = 0

    â„‹ = np.empty(N, dtype=np.float64)
    for k in range(N):
        U_1 = rnd.rand()
        U_2 = rnd.rand()

        # Technically the following works, but without @njit
        # it will print out "RuntimeWarning: invalid value encountered in log".
        # This is because 1 + Î²/(Î»Ë£_k + Î± - Î»)*np.log(U_2) can be negative
        # so T_2 can be np.NaN. The Dassios & Zhao (2013) algorithm checks if this
        # expression is negative and handles it separately, though the lines
        # below have the same behaviour as t_k = min(T_1, np.NaN) will be T_1. 
        T_1 = t_k - np.log(U_1) / Î»
        T_2 = t_k - np.log(1 + Î²/(Î»Ë£_k + Î± - Î»)*np.log(U_2))/Î²

        t_prev = t_k
        t_k = min(T_1, T_2)
        â„‹[k] = t_k

        if k > 0:
            Î»Ë£_k = Î» + (Î»Ë£_k + Î± - Î») * (
                np.exp(-Î² * (t_k - t_prev)))
        else:
            Î»Ë£_k = Î»
          
    return â„‹


@njit(nogil=True)
def exp_simulate_by_thinning(ğ›‰, T):
    Î», Î±, Î² = ğ›‰

    Î»Ë£ = Î»
    times = []

    t = 0

    while True:
        M = Î»Ë£
        Î”t = rnd.exponential() / M
        t += Î”t
        if t > T:
            break

        Î»Ë£ = Î» + (Î»Ë£ - Î») * np.exp(-Î² * Î”t)

        u = M * rnd.rand()
        if u > Î»Ë£:
            continue  # This potential arrival is 'thinned' out

        times.append(t)
        Î»Ë£ += Î±

    return np.array(times)


@njit(nogil=True)
def power_simulate_by_thinning(ğ›‰, T):
    Î», k, c, p = ğ›‰

    Î»Ë£ = Î»
    times = []

    t = 0

    while True:
        M = Î»Ë£
        Î”t = rnd.exponential() / M
        t += Î”t
        if t > T:
            break

        Î»Ë£ = power_hawkes_intensity(t, np.array(times), ğ›‰)

        u = M * rnd.rand()
        if u > Î»Ë£:
            continue  # This potential arrival is 'thinned' out

        times.append(t)
        Î»Ë£ += k / (c ** p)

    return np.array(times)


# Moment matching


def empirical_moments(ğ­, T, Ï„, lag):
    bins = np.arange(0, T, Ï„)
    N = len(bins) - 1
    count = np.zeros(N)

    for i in range(N):
        count[i] = np.sum((bins[i] <= ğ­) & (ğ­ < bins[i+1]))

    empMean = np.mean(count)
    empVar = np.std(count)**2
    empAutoCov = np.mean((count[:-lag] - empMean) \
                    * (count[lag:] - empMean))

    return np.array([empMean, empVar, empAutoCov]).reshape(3,1)



def exp_moments(ğ›‰, Ï„, lag):
    """
    Consider an exponential Hawkes process with parameter ğ›‰.
    Look at intervals of length Ï„, i.e. N(t+Ï„) - N(t).
    Calculate the limiting (t->âˆ) mean and variance.
    Also, get the limiting autocovariance:
        E[ (N(t + Ï„) - N(t)) (N(t + lag*Ï„ + Ï„) - N(t + lag*Ï„)) ].
    """
    Î», Î±, Î² = ğ›‰
    Îº = Î² - Î±
    Î´ = lag*Ï„

    mean = (Î»*Î²/Îº)*Ï„
    var = (Î»*Î²/Îº)*(Ï„*(Î²/Îº) + (1 - Î²/Îº)*((1 - np.exp(-Îº*Ï„))/Îº))
    autoCov = (Î»*Î²*Î±*(2*Î²-Î±)*(np.exp(-Îº*Ï„) - 1)**2/(2*Îº**4)) \
                *np.exp(-Îº*Î´)

    return np.array([mean, var, autoCov]).reshape(3,1)


def exp_gmm_loss(ğ›‰, Ï„, lag, empMoments, W):
    moments = exp_moments(ğ›‰, Ï„, lag)
    ğ  = empMoments - moments
    return (ğ .T).dot(W).dot(ğ )[0,0]

def exp_gmm(ğ­, T, Ï„=5, lag=5, iters=2, ğ›‰_start=np.array([1.0, 2.0, 3.0])):
    empMoments = empirical_moments(ğ­, T, Ï„, lag)

    W = np.eye(3)
    bounds = ((0, None), (0, None), (0, None))

    ğ›‰ = minimize(exp_gmm_loss, x0=ğ›‰_start,
            args=(Ï„, lag, empMoments, W),
            bounds=bounds).x

    for i in range(iters):
        moments = exp_moments(ğ›‰, Ï„, lag)

        ğ  = empMoments - moments
        S = ğ .dot(ğ .T)

        W = np.linalg.inv(S)
        W /= np.max(W) # Avoid overflow of the loss function

        ğ›‰ = minimize(exp_gmm_loss, x0=ğ›‰,
                args=(Ï„, lag, empMoments, W),
                bounds=bounds).x

    return ğ›‰


# Fit EM


@njit(nogil=True, parallel=True)
def em_responsibilities(ğ­, ğ›‰):
    Î», Î±, Î² = ğ›‰

    N = len(ğ­)
    resp = np.empty((N,N), dtype=np.float64)

    for i in prange(0,N):
        if i == 0:
            resp[i, 0] = 1.0
            for j in range(1, N):
                resp[i, j] = 0.0
        else:
            resp[i, 0] = Î»
            rowSum = Î»

            for j in range(1, i+1):
                resp[i, j] = Î±*np.exp(-Î²*(ğ­[i] - ğ­[j-1]))
                rowSum += resp[i, j]

            for j in range(0, i+1):
                resp[i, j] /= rowSum

            for j in range(i+1, N):
                resp[i, j] = 0.0
    return resp


def exp_em(ğ­, T, ğ›‰_start=np.array([1.0, 2.0, 3.0]), iters=100, verbosity=None, calcLikelihoods=False):
    """
    Run an EM fit on the 'ğ­' arrival times up until final time 'T'.
    """
    ğ›‰ = ğ›‰_start.copy()

    llIterations = np.zeros(iters)
    iters = tqdm(range(iters)) if verbosity else range(iters)

    for i in iters:
        ğ›‰, ll = exp_em_iter(ğ­, T, ğ›‰, calcLikelihoods)
        llIterations[i] = ll

        if verbosity and i % verbosity == 0:
            print(ğ›‰[0], ğ›‰[1], ğ›‰[2])

    if calcLikelihoods:
        return ğ›‰, llIterations
    else:
        return ğ›‰


@njit(nogil=True, parallel=True)
def exp_em_iter(ğ­, T, ğ›‰, calcLikelihoods):
    Î», Î±, Î² = ğ›‰
    N = len(ğ­)

    # E step
    resp = em_responsibilities(ğ­, ğ›‰)

    # M step: Update Î»
    Î» = np.sum(resp[:,0])/T

    # M step: Update Î±
    numer = np.sum(resp[:,1:])
    denom = np.sum(1 - np.exp(-Î²*(T - ğ­)))
    Î± = Î²*numer/denom

    # M step: Update Î²
    numer = np.sum(1 - np.exp(-Î²*(T - ğ­)))/Î² - np.sum((T - ğ­)*np.exp(-Î²*(T - ğ­)))

    denom = 0
    for j in prange(1, N):
        denom += np.sum((ğ­[j] - ğ­[:j])*resp[j,1:j+1])

    Î² = Î±*numer/denom

    if calcLikelihoods:
        ll = exp_log_likelihood(ğ­, T, ğ›‰)
    else:
        ll = 0.0

    ğ›‰[0] = Î»
    ğ›‰[1] = Î±
    ğ›‰[2] = Î²

    return ğ›‰, ll


## Mutually exciting Hawkes with exponential decay
@njit()
def mutual_hawkes_intensity(t, â„‹_t, ğ›‰):
    """
    Each Î¼[i] is an m-vector-valued function, which takes as argument
    the time passed since an arrival to process i, and returns the
    lasting effect on each of the m processes
    """
    Î», Î¼ = ğ›‰

    Î»Ë£ = Î»
    for (t_i, d_i) in â„‹_t:
        Î»Ë£ += Î¼[d_i](t - t_i)
    return Î»Ë£


@njit(nogil=True)
def mutual_exp_hawkes_intensity(t, times, ids, ğ›‰):
    """
    The Î» is an m-vector which shows the starting intensity for
    each process.

    Each Î±[i] is an m-vector which shows the jump in intensity
    for each of the processes when an arrival comes to process i.

    The Î² is an m-vector which shows the intensity decay rates for
    each processes intensity.
    """
    Î», Î±, Î² = ğ›‰

    Î»Ë£ = Î».copy()
    for (t_i, d_i) in zip(times, ids):
        Î»Ë£ += Î±[d_i] * np.exp(-Î² * (t - t_i))

    return Î»Ë£


@njit(nogil=True)
def mutual_exp_hawkes_compensator(t, times, ids, ğ›‰):
    """
    The Î» is an m-vector which shows the starting intensity for
    each process.

    Each Î±[i] is an m-vector which shows the jump in intensity
    for each of the processes when an arrival comes to process i.

    The Î² is an m-vector which shows the intensity decay rates for
    each processes intensity.
    """
    # if t <= 0: return np.zeros(m)

    Î», Î±, Î² = ğ›‰

    Î› = Î» * t

    for (t_i, d_i) in zip(times, ids):
        # Î› += M(t - t_i, d_i)
        Î› += (Î±[d_i]/Î²) * (1 - np.exp(-Î²*(t - t_i)))
    return Î›


@njit(nogil=True)
def mutual_exp_hawkes_compensators(times, ids, ğ›‰):
    """
    The Î» is an m-vector which shows the starting intensity for
    each process.

    Each Î±[i] is an m-vector which shows the jump in intensity
    for each of the processes when an arrival comes to process i.

    The Î² is an m-vector which shows the intensity decay rates for
    each processes intensity.
    """

    Î», Î±, Î² = ğ›‰
    m = len(Î»)

    Î› = np.zeros(m)
    Î»Ë£_prev = Î»
    t_prev = 0

    Î›s = np.zeros((len(times), m), dtype=np.float64)

    for i in range(len(times)):
        t_i = times[i]
        d_i = ids[i]

        Î› += Î» * (t_i - t_prev) + (Î»Ë£_prev - Î»)/Î² * (1 - np.exp(-Î²*(t_i - t_prev)))
        Î›s[i,:] = Î›

        Î»Ë£_prev = Î» + (Î»Ë£_prev - Î») * np.exp(-Î² * (t_i - t_prev)) + Î±[d_i,:]
        t_prev = t_i

    return Î›s


@njit(nogil=True)
def mutual_log_likelihood(â„‹_T, T, ğ›‰, Î»Ë£, Î›):
    m = len(ğ›‰)
    â„“ = 0
    for (t_i, d_i) in â„‹_T:
        if t_i > T:
            raise RuntimeError("T is too small for this data")

        # Get the history of arrivals before time t_i
        â„‹_i = [(t_s, d_s) for (t_s, d_s) in â„‹_T if t_s < t_i]
        Î»Ë£_i = Î»Ë£(t_i, â„‹_i, ğ›‰)
        â„“ += np.log(Î»Ë£_i[d_i])

    â„“ -= np.sum(Î›(T, â„‹_T, ğ›‰))
    return â„“


@njit(nogil=True)
def mutual_exp_log_likelihood(times, ids, T, ğ›‰):
    if np.max(times) > T:
        raise RuntimeError("T is too small for this data")

    Î», Î±, Î² = ğ›‰

    if np.min(Î») <= 0 or np.min(Î±) < 0 or np.min(Î²) <= 0: return -np.inf

    â„“ = 0
    Î»Ë£ = ğ›‰[0]

    t_prev = 0
    for t_i, d_i in zip(times, ids):
        Î»Ë£ = Î» + (Î»Ë£ - Î») * np.exp(-Î² * (t_i - t_prev))
        â„“ += np.log(Î»Ë£[d_i])

        Î»Ë£ += Î±[d_i,:]
        t_prev = t_i

    â„“ -= np.sum(mutual_exp_hawkes_compensator(T, times, ids, ğ›‰))

    return â„“


def mutual_exp_simulate_by_thinning(ğ›‰, T):

    """
    The Î» is an m-vector which shows the starting intensity for
    each process.

    Each Î±[i] is an m-vector which shows the jump in intensity
    for each of the processes when an arrival comes to process i.

    The Î² is an m-vector which shows the intensity decay rates for
    each processes intensity.
    """
    Î», Î±, Î² = ğ›‰
    m = len(Î»)

    Î»Ë£ = Î»
    times = []

    t = 0

    while True:
        M = np.sum(Î»Ë£)
        Î”t = rnd.exponential() / M
        t += Î”t
        if t > T:
            break

        Î»Ë£ = Î» + (Î»Ë£ - Î») * np.exp(-Î² * Î”t)

        u = M * rnd.rand()
        if u > np.sum(Î»Ë£):
            continue # No arrivals (they are 'thinned' out)

        cumulativeÎ»Ë£ = 0

        for i in range(m):
            cumulativeÎ»Ë£ += Î»Ë£[i]
            if u < cumulativeÎ»Ë£:
                times.append((t, i))
                Î»Ë£ += Î±[i]
                break

    return times


def flatten_theta(ğ›‰):
    return np.hstack([ğ›‰[0], np.hstack(ğ›‰[1]), ğ›‰[2]])


def unflatten_theta(ğ›‰_flat, m):
    Î» = ğ›‰_flat[:m]
    Î± = ğ›‰_flat[m:(m + m**2)].reshape((m,m))
    Î² = ğ›‰_flat[(m + m**2):]

    return (Î», Î±, Î²)


def mutual_exp_mle(ğ­, ids, T, ğ›‰_start):

    m = len(ğ›‰_start[0])
    ğ›‰_start_flat = flatten_theta(ğ›‰_start)

    def loss(ğ›‰_flat):
        return -mutual_exp_log_likelihood(ğ­, ids, T, unflatten_theta(ğ›‰_flat, m))

    def print_progress(ğ›‰_i, itCount = []):
        itCount.append(None)
        i = len(itCount)

        if i % 100 == 0:
            ll = -loss(ğ›‰_i)
            print(f"Iteration {i} loglikelihood {ll:.2f}")

    res = minimize(loss, ğ›‰_start_flat, options={"disp": True, "maxiter": 100_000},
        callback = print_progress, method = 'Nelder-Mead')

    ğ›‰_mle = unflatten_theta(res.x, m)
    logLike = -res.fun

    return ğ›‰_mle, logLike


# More advanced MLE methods for the exponential case


@njit()
def ozaki_recursion(ğ­, ğ›‰, n):
    """
    Calculate sum_{j=1}^{i-1} t_j^n * exp(-Î² * (t_i - t_j)) recursively
    """
    Î», Î±, Î² = ğ›‰
    N_T = len(ğ­)

    A_n = np.empty(N_T, dtype=np.float64)
    A_n[0] = 0
    for i in range(1, N_T):
        A_n[i] = np.exp(-Î²*(ğ­[i] - ğ­[i-1])) * (ğ­[i-1]**n + A_n[i-1])

    return A_n


@njit()
def deriv_exp_log_likelihood(â„‹_T, T, ğ›‰):
    Î», Î±, Î² = ğ›‰

    ğ­ = â„‹_T
    N_T = len(ğ­)

    A = ozaki_recursion(ğ­, ğ›‰, 0)
    A_1 = ozaki_recursion(ğ­, ğ›‰, 1)

    B = np.empty(N_T, dtype=np.float64)
    B[0] = 0

    for i in range(1, N_T):
        B[i] = ğ­[i] * A[i] - A_1[i]

    dâ„“dÎ» = -T
    dâ„“dÎ± = 0
    dâ„“dÎ² = 0

    for i, t_i in enumerate(â„‹_T):
        dâ„“dÎ± += (1/Î²) * (np.exp(-Î²*(T-t_i)) - 1) + A[i] / (Î» + Î± * A[i])
        dâ„“dÎ² += -Î± * ( (1/Î²) * (T - t_i) * np.exp(-Î²*(T-t_i)) \
                     + (1/Î²**2) * (np.exp(-Î²*(T-t_i))-1) ) \
                - (Î± * B[i] / (Î» + Î± * A[i]))
        dâ„“dÎ» += 1 / (Î» + Î± * A[i])

    d = np.empty(3, dtype=np.float64)
    d[0] = dâ„“dÎ»
    d[1] = dâ„“dÎ±
    d[2] = dâ„“dÎ²
    return d


@njit()
def hess_exp_log_likelihood(â„‹_T, T, ğ›‰):
    Î», Î±, Î² = ğ›‰

    ğ­ = â„‹_T
    N_T = len(ğ­)

    A = ozaki_recursion(ğ­, ğ›‰, 0)
    A_1 = ozaki_recursion(ğ­, ğ›‰, 1)
    A_2 = ozaki_recursion(ğ­, ğ›‰, 2)

    # B is sum (t_i - t_j) * exp(- ...)
    # C is sum (t_i - t_j)**2 * exp(- ...)
    B = np.empty(N_T, dtype=np.float64)
    C = np.empty(N_T, dtype=np.float64)
    B[0] = 0
    C[0] = 0

    for i in range(1, N_T):
        B[i] = ğ­[i] * A[i] - A_1[i]
        C[i] = ğ­[i]**2 * A[i] - 2*ğ­[i]*A_1[i] + A_2[i]

    d2â„“dÎ±2 = 0
    d2â„“dÎ±dÎ² = 0
    d2â„“dÎ²2 = 0

    d2â„“dÎ»2 = 0
    d2â„“dÎ±dÎ» = 0
    d2â„“dÎ²dÎ» = 0

    for i, t_i in enumerate(â„‹_T):
        d2â„“dÎ±2 += - ( A[i] / (Î» + Î± * A[i]) )**2
        d2â„“dÎ±dÎ² += - ( (1/Î²) * (T - t_i) * np.exp(-Î²*(T-t_i)) \
                     + (1/Î²**2) * (np.exp(-Î²*(T-t_i))-1) ) \
                   + ( -B[i]/(Î» + Î± * A[i]) + (Î± * A[i] * B[i]) / (Î» + Î± * A[i])**2 )

        d2â„“dÎ²2 += Î± * ( (1/Î²) * (T - t_i)**2 * np.exp(-Î²*(T-t_i)) + \
                        (2/Î²**2) * (T - t_i) * np.exp(-Î²*(T-t_i)) + \
                        (2/Î²**3) * (np.exp(-Î²*(T-t_i)) - 1) ) + \
                  ( Î±*C[i] / (Î» + Î± * A[i]) - (Î±*B[i] / (Î» + Î± * A[i]))**2 )


        d2â„“dÎ»2 += -1 / (Î» + Î± * A[i])**2
        d2â„“dÎ±dÎ» += -A[i] / (Î» + Î± * A[i])**2
        d2â„“dÎ²dÎ» += Î± * B[i] / (Î» + Î± * A[i])**2

    H = np.empty((3,3), dtype=np.float64)
    H[0,0] = d2â„“dÎ»2
    H[1,1] = d2â„“dÎ±2
    H[2,2] = d2â„“dÎ²2
    H[0,1] = H[1,0] = d2â„“dÎ±dÎ»
    H[0,2] = H[2,0] = d2â„“dÎ²dÎ»
    H[1,2] = H[2,1] = d2â„“dÎ±dÎ²
    return H


def exp_mle_with_grad(ğ­, T, ğ›‰_start=np.array([1.0, 2.0, 3.0])):
    eps = 1e-5
    ğ›‰_bounds = ((eps, None), (eps, None), (eps, None))
    loss = lambda ğ›‰: -exp_log_likelihood(ğ­, T, ğ›‰)
    grad = lambda ğ›‰: -deriv_exp_log_likelihood(ğ­, T, ğ›‰)
    ğ›‰_mle = minimize(loss, ğ›‰_start, bounds=ğ›‰_bounds, jac=grad).x

    return ğ›‰_mle


def exp_mle_with_hess(ğ­, T, ğ›‰_start=np.array([1.0, 2.0, 3.0])):
    eps = 1e-5
    ğ›‰_bounds = ((eps, None), (eps, None), (eps, None))
    loss = lambda ğ›‰: -exp_log_likelihood(ğ­, T, ğ›‰)
    grad = lambda ğ›‰: -deriv_exp_log_likelihood(ğ­, T, ğ›‰)
    hess = lambda ğ›‰: -hess_exp_log_likelihood(ğ­, T, ğ›‰)
    ğ›‰_mle = minimize(loss, ğ›‰_start, bounds=ğ›‰_bounds, jac=grad, hess=hess,
        method="trust-constr").x

    return ğ›‰_mle


# Alternative simulation method


@njit(nogil=True)
def exp_simulate_by_composition_alt(ğ›‰, T):
    """
    This is simply an alternative to 'exp_simulate_by_composition'
    where the simulation stops after time T rather than stopping after
    observing N arrivals.
    """
    Î», Î±, Î² = ğ›‰
    Î»Ë£_k = Î»
    t_k = 0

    â„‹ = []
    while t_k < T:
        U_1 = rnd.rand()
        U_2 = rnd.rand()

        # Technically the following works, but without @njit
        # it will print out "RuntimeWarning: invalid value encountered in log".
        # This is because 1 + Î²/(Î»Ë£_k + Î± - Î»)*np.log(U_2) can be negative
        # so T_2 can be np.NaN. The Dassios & Zhao (2013) algorithm checks if this
        # expression is negative and handles it separately, though the lines
        # below have the same behaviour as t_k = min(T_1, np.NaN) will be T_1. 
        T_1 = t_k - np.log(U_1) / Î»
        T_2 = t_k - np.log(1 + Î²/(Î»Ë£_k + Î± - Î»)*np.log(U_2))/Î²

        t_prev = t_k
        t_k = min(T_1, T_2)
        â„‹.append(t_k)

        if len(â„‹) > 1:
            Î»Ë£_k = Î» + (Î»Ë£_k + Î± - Î») * (
                    np.exp(-Î² * (t_k - t_prev)))
        else:
            Î»Ë£_k = Î»

    return np.array(â„‹[:-1])





# Estimate hawkes processes using the toxicity parameter 

def exp_mle_toxicity(â„‹_T,ğ’¯_T, T,ğ›‰_start=np.array([0.0001, 0.0001,.00001,.00001])):
    eps = 0.00001e-300
    ğ›‰_bounds = ((eps, None), (eps, None), (eps, None),(eps, None))
    loss = lambda ğ›‰: -exp_log_likelihood_toxicity(â„‹_T,ğ’¯_T, T, ğ›‰)
    ğ›‰_mle = minimize(loss, ğ›‰_start, bounds=ğ›‰_bounds).x
    return np.array(ğ›‰_mle)

def exp_log_likelihood_toxicity(â„‹_T,ğ’¯_T, T, ğ›‰):
    Î», Î±_1,Î±_2, Î² = ğ›‰
    ğ­ = â„‹_T
    N_T = len(ğ­)

    A = np.empty(N_T, dtype=np.float64)
    A[0] = 0
    for i in range(1, N_T):
        A[i] = np.exp(-Î²*(ğ­[i] - ğ­[i-1])) * (1 + A[i-1])

    â„“ = -Î»*T
    for i, t_i in enumerate(â„‹_T):
        â„“ += np.log(Î» + (Î±_1 + Î±_2 * ğ’¯_T[i]) * A[i]) - \
                ((Î±_1 + Î±_2 * ğ’¯_T[i])/Î²) * (1 - np.exp(-Î²*(T-t_i)))
    return â„“

def exp_hawkes_compensators_toxicity(â„‹_t,ğ’¯_T, ğ›‰):
    Î», Î±_1,Î±_2, Î² = ğ›‰

    Î› = 0
    Î»Ë£_prev = Î»
    t_prev = 0

    Î›s = np.empty(len(â„‹_t), dtype=np.float64)
    for i, t_i in enumerate(â„‹_t):
        Î› += Î» * (t_i - t_prev) + (
                (Î»Ë£_prev - Î»)/Î² *
                (1 - np.exp(-Î²*(t_i - t_prev))))
        Î›s[i] = Î›

        Î»Ë£_prev = Î» + (Î»Ë£_prev - Î») * (
                np.exp(-Î² * (t_i - t_prev))) + (Î±_1 + Î±_2 *(ğ’¯_T[i]>0.5))
        t_prev = t_i
    return Î›s

### Simulate colllecive behaviour 
import pandas as pd

def simulate_hawkes_collective_behaviour(root,dataset,alpha,beta,grid_search_results):
  parameter_pool=grid_search_results
  root=dataset[dataset['root_submission']=='0']
  user_activity=root['user'].value_counts().reset_index()
  active_users=user_activity[user_activity['count']>2]['user']
  root=root[root['user'].isin(active_users)]
  user_activity=root['user'].value_counts().reset_index()

  parameter_pool=parameter_pool[(parameter_pool['beta']==beta) & (parameter_pool['alpha']<=alpha) ]

  root.sort_values(by='created_at', inplace=True)

  observed_data = np.array([np.datetime64(x.replace(tzinfo=None)).astype(np.int64) for x in root['created_at']])
  start_conversation=np.datetime64(min(root['created_at']).replace(tzinfo=None))
  end_conversation=np.datetime64(max(root['created_at']).replace(tzinfo=None))

  â„‹_t = (observed_data - start_conversation.astype(np.int64)) / (end_conversation.astype(np.int64) - start_conversation.astype(np.int64))

  first_comments_table = root.groupby('user')['created_at'].first().reset_index()
  first_comments = np.array([np.datetime64(x.replace(tzinfo=None)).astype(np.int64) for x in first_comments_table['created_at']])
  oss_staring_conversation = (first_comments - start_conversation.astype(np.int64)) / (end_conversation.astype(np.int64) - start_conversation.astype(np.int64))
  first_comments_table['created_at']=oss_staring_conversation

  last_comments_table = root.groupby('user')['created_at'].last().reset_index()
  last_comments = np.array([np.datetime64(x.replace(tzinfo=None)).astype(np.int64) for x in last_comments_table['created_at']])
  oss_finishing_conversation = (last_comments - start_conversation.astype(np.int64)) / (end_conversation.astype(np.int64) - start_conversation.astype(np.int64))
  last_comments_table['created_at']=oss_finishing_conversation



  simulated_thread_df = pd.DataFrame(columns=['user', 'timestamp'])  # Creiamo il DataFrame vuoto

  for index, row in (user_activity.iterrows()):
    user_name = row['user']
    number_of_comments = row['count']
    t_init=first_comments_table[first_comments_table['user']==user_name]['created_at'].iloc[0]
    t_final=last_comments_table[last_comments_table['user']==user_name]['created_at'].iloc[0]
    parameters_cool = parameter_pool[parameter_pool['mu_expected_value'].round() == number_of_comments-2]
    i=1
    while len(parameters_cool)==0:
      i+=1
      parameters_cool = parameter_pool.query(f"({number_of_comments - i} <= mu_expected_value.round() <= {number_of_comments + i})")
    parameters_cool = parameters_cool.loc[parameters_cool['alpha'].idxmax()]
    theta = np.array(parameters_cool[['lambda', 'alpha', 'beta']])

    simulated_timestamp = exp_simulate_by_composition_alt(theta, 1)
    simulated_timestamp=(simulated_timestamp*(t_final-t_init))+t_init
    simulated_timestamp= np.concatenate(([t_init], simulated_timestamp,[t_final]))
    user_df = pd.DataFrame({'user': [user_name] * (len(simulated_timestamp)), 'timestamp': simulated_timestamp})
    simulated_thread_df = pd.concat([simulated_thread_df, user_df], ignore_index=True)

  simulated_thread_df.sort_values(by='timestamp', inplace=True)
  â„‹_t_simulated=simulated_thread_df['timestamp']

  # Test with metrics


  grouped_counts = root.groupby('user')['comment_id'].count()
  selected_users = grouped_counts[grouped_counts > 5].reset_index().user

  root['time']=â„‹_t.round(2)
  root=root[root['user'].isin(selected_users)]
  simulated_thread_df=simulated_thread_df[simulated_thread_df['user'].isin(selected_users)]


  â„‹_t_simulated=simulated_thread_df['timestamp']
  â„‹_t=root['time']
  return â„‹_t,â„‹_t_simulated


# Ccreate cumulate distribution from timestamps
def F(x, H_t):
    cumulative_series = []
    for i in range(len(x)):
        count = 0
        for t in H_t:
            if t <= x[i]:
                count += 1
        cumulative_series.append(count)
    return cumulative_series






